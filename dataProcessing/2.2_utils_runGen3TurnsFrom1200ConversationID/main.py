"""
ğŸš€ FAST RESPONSE EVALUATION PIPELINE WITH AUTO MERGE & CLEANUP
=============================================================

ğŸ“‹ Má»¤C ÄÃCH:
- Tá»± Ä‘á»™ng hÃ³a quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡ Fast Response cho nhiá»u conversation IDs
- Xá»­ lÃ½ tá»« A-Z: Fetch data -> Process -> Evaluate -> Merge -> Cleanup
- Tá»± Ä‘á»™ng gá»™p táº¥t cáº£ káº¿t quáº£ vÃ o 1 file Excel duy nháº¥t
- Tá»± Ä‘á»™ng dá»n dáº¹p cÃ¡c file trung gian sau khi hoÃ n thÃ nh

ğŸ”„ QUY TRÃŒNH PIPELINE:
1. ğŸ“¥ Fetch: Láº¥y dá»¯ liá»‡u conversation tá»« API
2. âš™ï¸ Process: Xá»­ lÃ½ dá»¯ liá»‡u thÃ nh format chuáº©n
3. ğŸ¤– Evaluate: ÄÃ¡nh giÃ¡ vá»›i Fast Response API
4. ğŸ“Š Merge: Gá»™p táº¥t cáº£ káº¿t quáº£ vÃ o 1 file Excel
5. ğŸ—‘ï¸ Cleanup: XÃ³a cÃ¡c file trung gian, chá»‰ giá»¯ file merged

ğŸ“ Cáº¤U TRÃšC THÆ¯ Má»¤C:
- input/: Dá»¯ liá»‡u raw tá»« API
- output/: Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
- eval/: Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ tá»« Fast Response API
- final/: File Excel tá»•ng há»£p cuá»‘i cÃ¹ng (chá»‰ giá»¯ file merged)

ğŸ¯ CÃCH Sá»¬ Dá»¤NG:
python main_v2MergerClear.py --ids 358 359 362 --token your_token
python main_v2MergerClear.py --id_file ids.txt --token your_token

ğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG:
- 1 file Excel duy nháº¥t chá»©a táº¥t cáº£ conversation data
- Tá»± Ä‘á»™ng cÃ³ conversationID Ä‘á»ƒ phÃ¢n biá»‡t
- Tá»± Ä‘á»™ng sáº¯p xáº¿p theo ID tÄƒng dáº§n
- CÃ¡c file trung gian Ä‘Ã£ Ä‘Æ°á»£c dá»n dáº¹p
"""

import sys
import argparse
import os
import pandas as pd
import glob
import re
from datetime import datetime
from get_data_conversation import ConversationDataFetcher
from processed import ConversationProcessor
from run_eval_api_fast_response import FastResponseEvaluator

class FastResponsePipeline:
    def __init__(self, api_token: str):
        self.api_token = api_token
        self.fetcher = ConversationDataFetcher(api_token)
        self.processor = ConversationProcessor()
        self.evaluator = FastResponseEvaluator()
        
        # Táº¡o cÃ¡c folder cáº§n thiáº¿t
        for folder in ['input', 'output', 'eval', 'final']:
            if not os.path.exists(folder):
                os.makedirs(folder)
    
    def process_single_id(self, conversation_id: str) -> dict:
        """
        Xá»­ lÃ½ má»™t conversation ID
        """
        print(f"\n{'='*50}")
        print(f"ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ conversation ID: {conversation_id}")
        print(f"{'='*50}")
        
        results = {
            'id': conversation_id,
            'fetch_status': 'FAILED',
            'process_status': 'FAILED',
            'eval_status': 'FAILED',
            'input_file': '',
            'processed_file': '',
            'eval_file': ''
        }
        
        # BÆ°á»›c 1: Láº¥y dá»¯ liá»‡u
        print(f"ğŸ“¥ BÆ°á»›c 1: Láº¥y dá»¯ liá»‡u tá»« API...")
        input_file = self.fetcher.fetch_and_save(conversation_id)
        if input_file:
            results['fetch_status'] = 'SUCCESS'
            results['input_file'] = input_file
        else:
            print(f"âŒ KhÃ´ng thá»ƒ láº¥y dá»¯ liá»‡u cho ID: {conversation_id}")
            return results
        
        # BÆ°á»›c 2: Xá»­ lÃ½ dá»¯ liá»‡u
        print(f"âš™ï¸ BÆ°á»›c 2: Xá»­ lÃ½ dá»¯ liá»‡u...")
        processed_file = f"output/conversation_{conversation_id}_processed.xlsx"
        if self.processor.process_file(input_file, processed_file):
            results['process_status'] = 'SUCCESS'
            results['processed_file'] = processed_file
        else:
            print(f"âŒ KhÃ´ng thá»ƒ xá»­ lÃ½ dá»¯ liá»‡u cho ID: {conversation_id}")
            return results
        
        # BÆ°á»›c 3: ÄÃ¡nh giÃ¡ vá»›i API
        print(f"ğŸ¤– BÆ°á»›c 3: ÄÃ¡nh giÃ¡ vá»›i Fast Response API...")
        eval_file = f"eval/conversation_{conversation_id}_output_eval.xlsx"
        if self.evaluator.evaluate_excel_file(processed_file, eval_file):
            results['eval_status'] = 'SUCCESS'
            results['eval_file'] = eval_file
        else:
            print(f"âŒ KhÃ´ng thá»ƒ Ä‘Ã¡nh giÃ¡ cho ID: {conversation_id}")
            return results
        
        print(f"âœ… HoÃ n thÃ nh xá»­ lÃ½ ID: {conversation_id}")
        return results
    
    def calculate_avg_response_time(self, eval_file_path: str) -> float:
        """
        TÃ­nh response time trung bÃ¬nh tá»« file eval
        """
        try:
            if not os.path.exists(eval_file_path):
                return 0.0
                
            df = pd.read_excel(eval_file_path)
            
            # Kiá»ƒm tra xem cÃ³ cá»™t response_time khÃ´ng
            if 'response_time' not in df.columns:
                return 0.0
            
            # Xá»­ lÃ½ response_time, thay tháº¿ empty string báº±ng 0
            response_times = df['response_time'].replace('', 0).replace(None, 0)
            
            # Convert to numeric, errors='coerce' sáº½ chuyá»ƒn invalid values thÃ nh NaN
            response_times = pd.to_numeric(response_times, errors='coerce').fillna(0)
            
            # TÃ­nh trung bÃ¬nh, bá» qua cÃ¡c giÃ¡ trá»‹ 0
            valid_times = response_times[response_times > 0]
            if len(valid_times) > 0:
                return round(valid_times.mean(), 2)
            else:
                return 0.0
                
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi tÃ­nh avg response time cho {eval_file_path}: {e}")
            return 0.0

    def create_final_excel(self, results: list, output_file: str):
        """
        Táº¡o file Excel cuá»‘i cÃ¹ng vá»›i má»—i ID lÃ  má»™t sheet
        """
        print(f"\nğŸ“Š Táº¡o file Excel tá»•ng há»£p: {output_file}")
        
        # Táº¡o summary data trÆ°á»›c (Ä‘á»ƒ cÃ³ thá»ƒ dÃ¹ng trong exception)
        summary_data = []
        for result in results:
            summary_data.append({
                'ID': result['id'],
                'Fetch Status': result['fetch_status'],
                'Process Status': result['process_status'],
                'Eval Status': result['eval_status'],
                'Input File': result['input_file'],
                'Processed File': result['processed_file'],
                'Eval File': result['eval_file'],
                'Avg Response Time (ms)': self.calculate_avg_response_time(result['eval_file']) if result['eval_status'] == 'SUCCESS' else 0
            })
        
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # Táº¡o sheet tá»•ng quan
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
                print(f"âœ… ÄÃ£ táº¡o sheet Summary")
                
                # Táº¡o sheet cho tá»«ng ID
                successful_sheets = 0
                for result in results:
                    if result['eval_status'] == 'SUCCESS' and os.path.exists(result['eval_file']):
                        try:
                            df = pd.read_excel(result['eval_file'])
                            sheet_name = f"ID_{result['id']}"
                            
                            # Kiá»ƒm tra Ä‘á»™ dÃ i tÃªn sheet (Excel limit 31 chars)
                            if len(sheet_name) > 31:
                                sheet_name = sheet_name[:31]
                            
                            df.to_excel(writer, sheet_name=sheet_name, index=False)
                            print(f"âœ… ÄÃ£ thÃªm sheet: {sheet_name}")
                            successful_sheets += 1
                            
                        except Exception as e:
                            print(f"âŒ Lá»—i khi thÃªm sheet cho ID {result['id']}: {e}")
                    else:
                        if result['eval_status'] == 'SUCCESS':
                            print(f"âš ï¸ File khÃ´ng tá»“n táº¡i: {result['eval_file']}")
                
                print(f"ğŸ“Š Tá»•ng cá»™ng: {successful_sheets + 1} sheets")
            
            print(f"âœ… ÄÃ£ táº¡o file tá»•ng há»£p: {output_file}")
            
        except Exception as e:
            print(f"âŒ Lá»—i khi táº¡o file Excel tá»•ng há»£p: {e}")
            print(f"ğŸ” Loáº¡i lá»—i: {type(e).__name__}")
            
            # Táº¡o file backup Ä‘Æ¡n giáº£n
            try:
                backup_file = output_file.replace('.xlsx', '_backup.csv')
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_csv(backup_file, index=False, encoding='utf-8')
                print(f"ğŸ“„ ÄÃ£ táº¡o file backup CSV: {backup_file}")
            except Exception as backup_error:
                print(f"âŒ KhÃ´ng thá»ƒ táº¡o backup: {backup_error}")
    
    def run_pipeline(self, conversation_ids: list):
        """
        Cháº¡y pipeline cho danh sÃ¡ch conversation IDs vá»›i Auto Merge & Cleanup
        """
        print(f"ğŸš€ Báº¯t Ä‘áº§u FULL PIPELINE vá»›i {len(conversation_ids)} conversation IDs")
        print(f"ğŸ“‹ Danh sÃ¡ch IDs: {', '.join(conversation_ids)}")
        
        results = []
        
        # BÆ°á»›c 1-3: Xá»­ lÃ½ tá»«ng ID (Fetch -> Process -> Evaluate)
        print(f"\n{'='*60}")
        print(f"ğŸ”„ BÆ¯á»šC 1-3: Xá»¬ LÃ Tá»ªNG CONVERSATION ID")
        print(f"{'='*60}")
        
        for conv_id in conversation_ids:
            result = self.process_single_id(conv_id)
            results.append(result)
        
        # In bÃ¡o cÃ¡o tÃ³m táº¯t pipeline
        self.print_summary_report(results)
        
        # BÆ°á»›c 4: Merge táº¥t cáº£ file Excel thÃ nh 1 file duy nháº¥t
        print(f"\n{'='*60}")
        print(f"ğŸ“Š BÆ¯á»šC 4: MERGE Táº¤T Cáº¢ FILE EXCEL")
        print(f"{'='*60}")
        
        merged_file = self.merge_all_excel_files()
        
        if merged_file:
            # BÆ°á»›c 5: Cleanup - XÃ³a táº¥t cáº£ file trung gian
            print(f"\n{'='*60}")
            print(f"ğŸ—‘ï¸ BÆ¯á»šC 5: CLEANUP FILE TRUNG GIAN")
            print(f"{'='*60}")
            
            self.cleanup_intermediate_files(merged_file)
            
            # In bÃ¡o cÃ¡o cuá»‘i cÃ¹ng
            print(f"\n{'='*60}")
            print(f"ğŸ‰ PIPELINE HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
            print(f"{'='*60}")
            print(f"ğŸ“Š Tá»•ng conversation IDs: {len(conversation_ids)}")
            print(f"âœ… IDs thÃ nh cÃ´ng: {sum(1 for r in results if r['eval_status'] == 'SUCCESS')}")
            print(f"ğŸ“ File káº¿t quáº£ cuá»‘i cÃ¹ng: {merged_file}")
            print(f"ğŸ—‘ï¸ ÄÃ£ dá»n dáº¹p táº¥t cáº£ file trung gian")
            print(f"{'='*60}")
            
        else:
            print(f"\nâŒ PIPELINE THáº¤T Báº I: KhÃ´ng thá»ƒ merge files!")
        
        return results, merged_file if merged_file else None
    
    def extract_conversation_id(self, filename: str) -> int:
        """
        Extract conversation ID from filename
        Example: conversation_19720_output_eval.xlsx -> 19720
        """
        match = re.search(r'conversation_(\d+)_output_eval\.xlsx', filename)
        if match:
            return int(match.group(1))
        return None

    def count_xlsx_files(self, folder_path: str) -> tuple:
        """
        Äáº¿m sá»‘ file .xlsx trong folder chá»‰ Ä‘á»‹nh
        """
        xlsx_pattern = os.path.join(folder_path, "*.xlsx")
        xlsx_files = glob.glob(xlsx_pattern)
        count = len(xlsx_files)
        
        print(f"ğŸ“Š THá»NG KÃŠ FILE XLSX - {os.path.basename(folder_path)}/")
        print(f"   Sá»‘ file .xlsx: {count}")
        
        if count > 0:
            total_size = sum(os.path.getsize(f) for f in xlsx_files)
            print(f"   Tá»•ng dung lÆ°á»£ng: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)")
        
        return count, xlsx_files

    def merge_all_excel_files(self) -> str:
        """
        Gá»™p táº¥t cáº£ file evaluation Excel vÃ o 1 file duy nháº¥t
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Báº®T Äáº¦U MERGE Táº¤T Cáº¢ FILE EXCEL")
        print(f"{'='*60}")
        
        # TÃ¬m táº¥t cáº£ file conversation_*_output_eval.xlsx trong folder eval/
        eval_pattern = os.path.join("eval", "conversation_*_output_eval.xlsx")
        xlsx_files = glob.glob(eval_pattern)
        
        # Debug: Hiá»ƒn thá»‹ táº¥t cáº£ files Ä‘Æ°á»£c tÃ¬m tháº¥y
        print(f"ğŸ” DEBUG: Pattern tÃ¬m kiáº¿m: {eval_pattern}")
        print(f"ğŸ” DEBUG: Sá»‘ file tÃ¬m tháº¥y: {len(xlsx_files)}")
        
        if xlsx_files:
            print(f"ğŸ“ Danh sÃ¡ch files tÃ¬m tháº¥y:")
            for i, file_path in enumerate(xlsx_files, 1):
                filename = os.path.basename(file_path)
                file_size = os.path.getsize(file_path)
                print(f"   {i}. {filename} ({file_size:,} bytes)")
        else:
            # Debug: Kiá»ƒm tra folder eval/ cÃ³ tá»“n táº¡i khÃ´ng
            if not os.path.exists("eval"):
                print("âŒ Folder eval/ khÃ´ng tá»“n táº¡i!")
            else:
                # Liá»‡t kÃª táº¥t cáº£ files trong eval/
                all_files = os.listdir("eval")
                print(f"ğŸ” DEBUG: Folder eval/ cÃ³ {len(all_files)} files:")
                for f in all_files:
                    print(f"   - {f}")
            return None
        
        print(f"ğŸ“ TÃ¬m tháº¥y {len(xlsx_files)} file Ä‘á»ƒ merge tá»« eval/")
        
        all_data = []
        error_files = []
        processed_count = 0
        
        for file_path in xlsx_files:
            try:
                filename = os.path.basename(file_path)
                
                # Extract conversation ID tá»« tÃªn file
                conversation_id = self.extract_conversation_id(filename)
                if conversation_id is None:
                    print(f"âŒ KhÃ´ng thá»ƒ extract ID tá»«: {filename}")
                    error_files.append(filename)
                    continue
                
                # Äá»c file Excel
                df = pd.read_excel(file_path)
                
                # Debug: Hiá»ƒn thá»‹ thÃ´ng tin DataFrame
                if processed_count == 0:  # Chá»‰ show info cá»§a file Ä‘áº§u tiÃªn
                    print(f"ğŸ” DEBUG: Cáº¥u trÃºc file Ä‘áº§u tiÃªn ({filename}):")
                    print(f"   - Rows: {len(df)}")
                    print(f"   - Columns: {len(df.columns)}")
                    print(f"   - Column names: {list(df.columns)}")
                    if len(df) > 0:
                        print(f"   - Sample data (first row): {df.iloc[0].to_dict()}")
                
                # ThÃªm cá»™t conversationID vÃ o Ä‘áº§u
                df.insert(0, 'conversationID', conversation_id)
                
                # ThÃªm vÃ o list tá»•ng
                all_data.append(df)
                processed_count += 1
                
                # Log progress má»—i 20 file
                if processed_count % 20 == 0:
                    print(f"âœ… ÄÃ£ xá»­ lÃ½: {processed_count}/{len(xlsx_files)} files")
                    
            except Exception as e:
                print(f"âŒ Lá»—i khi Ä‘á»c {filename}: {str(e)}")
                error_files.append(filename)
                continue
        
        if not all_data:
            print("âŒ KhÃ´ng cÃ³ data nÃ o Ä‘á»ƒ merge!")
            return None
        
        # Validation: Kiá»ƒm tra táº¥t cáº£ file cÃ³ cÃ¹ng columns khÃ´ng
        print(f"ğŸ” DEBUG: Validating column consistency...")
        all_columns = [list(df.columns) for df in all_data]
        first_columns = all_columns[0]
        
        column_mismatch = False
        for i, cols in enumerate(all_columns[1:], 1):
            if cols != first_columns:
                print(f"âš ï¸ WARNING: File {i+1} cÃ³ columns khÃ¡c: {cols}")
                column_mismatch = True
        
        if not column_mismatch:
            print(f"âœ… Táº¥t cáº£ {len(all_data)} files cÃ³ cÃ¹ng column structure")
        
        # Gá»™p táº¥t cáº£ DataFrame
        print(f"ğŸ“Š Äang gá»™p {len(all_data)} DataFrames...")
        try:
            merged_df = pd.concat(all_data, ignore_index=True)
            print(f"âœ… Concat thÃ nh cÃ´ng!")
        except Exception as e:
            print(f"âŒ Lá»—i khi concat DataFrames: {e}")
            return None
        
        # Sáº¯p xáº¿p theo conversationID
        print(f"ğŸ“ˆ Äang sáº¯p xáº¿p theo conversationID...")
        merged_df = merged_df.sort_values('conversationID').reset_index(drop=True)
        
        # Debug: ThÃ´ng tin vá» merged DataFrame
        print(f"ğŸ” DEBUG: Merged DataFrame info:")
        print(f"   - Total rows: {len(merged_df):,}")
        print(f"   - Total columns: {len(merged_df.columns)}")
        print(f"   - Columns: {list(merged_df.columns)}")
        print(f"   - ConversationID range: {merged_df['conversationID'].min()} - {merged_df['conversationID'].max()}")
        print(f"   - Unique conversations: {merged_df['conversationID'].nunique()}")
        
        # Táº¡o tÃªn file output vá»›i timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"merged_all_conversations_{timestamp}.xlsx"
        output_path = os.path.join("final", output_filename)
        
        # LÆ°u file
        print(f"ğŸ’¾ Äang lÆ°u file: {output_filename}")
        try:
            merged_df.to_excel(output_path, index=False)
            print(f"âœ… LÆ°u file thÃ nh cÃ´ng!")
        except Exception as e:
            print(f"âŒ Lá»—i khi lÆ°u file: {e}")
            return None
        
        # Thá»‘ng kÃª káº¿t quáº£
        print(f"\nğŸ“Š Káº¾T QUáº¢ MERGE:")
        print(f"âœ… ThÃ nh cÃ´ng: {processed_count}/{len(xlsx_files)} files")
        print(f"âŒ Lá»—i: {len(error_files)} files")
        print(f"ğŸ“Š Tá»•ng rows: {len(merged_df):,}")
        print(f"ğŸ“‹ Tá»•ng columns: {len(merged_df.columns)}")
        print(f"ğŸ’¾ File Ä‘Ã£ lÆ°u: {output_path}")
        
        # Thá»‘ng kÃª conversation IDs
        print(f"\nğŸ“ˆ THá»NG KÃŠ CONVERSATION IDs:")
        print(f"   Min ID: {merged_df['conversationID'].min()}")
        print(f"   Max ID: {merged_df['conversationID'].max()}")
        print(f"   Unique IDs: {merged_df['conversationID'].nunique()}")
        
        if error_files:
            print(f"\nâŒ CÃC FILE Bá»Š Lá»–I:")
            for error_file in error_files:
                print(f"   - {error_file}")
        
        return output_path

    def cleanup_intermediate_files(self, keep_merged_file: str):
        """
        XÃ³a táº¥t cáº£ file trung gian, chá»‰ giá»¯ láº¡i file merged
        """
        print(f"\n{'='*60}")
        print(f"ğŸ—‘ï¸ Báº®T Äáº¦U Dá»ŒN Dáº¸P FILE TRUNG GIAN")
        print(f"{'='*60}")
        
        folders_to_clean = ['input', 'output', 'eval']
        total_deleted = 0
        total_size_freed = 0
        
        for folder in folders_to_clean:
            if not os.path.exists(folder):
                continue
                
            print(f"\nğŸ—‚ï¸ Dá»n dáº¹p folder: {folder}/")
            
            # Äáº¿m files trÆ°á»›c khi xÃ³a
            count, files = self.count_xlsx_files(folder)
            if count == 0:
                print(f"   âœ¨ Folder Ä‘Ã£ sáº¡ch!")
                continue
            
            # XÃ³a táº¥t cáº£ files trong folder
            deleted_count = 0
            for file_path in files:
                try:
                    file_size = os.path.getsize(file_path)
                    os.remove(file_path)
                    deleted_count += 1
                    total_deleted += 1
                    total_size_freed += file_size
                    print(f"   ğŸ—‘ï¸ ÄÃ£ xÃ³a: {os.path.basename(file_path)}")
                except Exception as e:
                    print(f"   âŒ KhÃ´ng xÃ³a Ä‘Æ°á»£c {os.path.basename(file_path)}: {e}")
            
            print(f"   âœ… ÄÃ£ xÃ³a {deleted_count}/{count} files")
        
        # Dá»n dáº¹p folder final, chá»‰ giá»¯ file merged má»›i nháº¥t
        print(f"\nğŸ—‚ï¸ Dá»n dáº¹p folder: final/")
        if os.path.exists("final"):
            final_files = glob.glob(os.path.join("final", "*.xlsx"))
            kept_file = os.path.basename(keep_merged_file)
            
            deleted_in_final = 0
            for file_path in final_files:
                if os.path.basename(file_path) != kept_file:
                    try:
                        file_size = os.path.getsize(file_path)
                        os.remove(file_path)
                        deleted_in_final += 1
                        total_deleted += 1
                        total_size_freed += file_size
                        print(f"   ğŸ—‘ï¸ ÄÃ£ xÃ³a: {os.path.basename(file_path)}")
                    except Exception as e:
                        print(f"   âŒ KhÃ´ng xÃ³a Ä‘Æ°á»£c {os.path.basename(file_path)}: {e}")
            
            if deleted_in_final == 0:
                print(f"   âœ¨ Chá»‰ cÃ³ file merged, khÃ´ng cáº§n xÃ³a!")
            else:
                print(f"   âœ… ÄÃ£ xÃ³a {deleted_in_final} file cÅ©")
        
        print(f"\nğŸ¯ Káº¾T QUáº¢ Dá»ŒN Dáº¸P:")
        print(f"   ğŸ—‘ï¸ Tá»•ng files Ä‘Ã£ xÃ³a: {total_deleted}")
        print(f"   ğŸ’¾ Dung lÆ°á»£ng giáº£i phÃ³ng: {total_size_freed:,} bytes ({total_size_freed/1024/1024:.2f} MB)")
        print(f"   ğŸ“ File Ä‘Æ°á»£c giá»¯ láº¡i: {keep_merged_file}")

    def print_summary_report(self, results: list):
        """
        In bÃ¡o cÃ¡o tÃ³m táº¯t
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š BÃO CÃO TÃ“M Táº®T")
        print(f"{'='*60}")
        
        total = len(results)
        success_fetch = sum(1 for r in results if r['fetch_status'] == 'SUCCESS')
        success_process = sum(1 for r in results if r['process_status'] == 'SUCCESS')
        success_eval = sum(1 for r in results if r['eval_status'] == 'SUCCESS')
        
        print(f"ğŸ“ˆ Tá»•ng sá»‘ IDs xá»­ lÃ½: {total}")
        print(f"ğŸ“¥ Fetch thÃ nh cÃ´ng: {success_fetch}/{total} ({success_fetch/total*100:.1f}%)")
        print(f"âš™ï¸ Process thÃ nh cÃ´ng: {success_process}/{total} ({success_process/total*100:.1f}%)")
        print(f"ğŸ¤– Eval thÃ nh cÃ´ng: {success_eval}/{total} ({success_eval/total*100:.1f}%)")
        
        # Chi tiáº¿t tá»«ng ID
        print(f"\nğŸ“‹ Chi tiáº¿t tá»«ng ID:")
        for result in results:
            status_icon = "âœ…" if result['eval_status'] == 'SUCCESS' else "âŒ"
            avg_time = self.calculate_avg_response_time(result['eval_file']) if result['eval_status'] == 'SUCCESS' else 0
            print(f"{status_icon} ID {result['id']}: {result['eval_status']} (Avg: {avg_time}ms)")
        
        # Files Ä‘Æ°á»£c táº¡o
        print(f"\nğŸ“ Files Ä‘Æ°á»£c táº¡o:")
        for result in results:
            if result['eval_status'] == 'SUCCESS':
                print(f"   ğŸ“„ {result['eval_file']}")

def parse_arguments():
    """
    Parse command line arguments
    """
    print("ğŸ” DEBUG: Parsing command line arguments...")
    print(f"ğŸ” DEBUG: sys.argv = {sys.argv}")
    
    parser = argparse.ArgumentParser(description='Fast Response Evaluation Pipeline')
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--ids', nargs='+', help='Conversation IDs (space separated)')
    group.add_argument('--id_file', type=str, help='File containing conversation IDs (one per line)')
    
    parser.add_argument('--token', type=str, default='{{token}}', 
                       help='API token (default: {{token}})')
    
    args = parser.parse_args()
    
    # Debug thÃ´ng tin arguments
    print(f"ğŸ” DEBUG: Parsed arguments:")
    print(f"ğŸ” DEBUG: args.ids = {getattr(args, 'ids', None)}")
    print(f"ğŸ” DEBUG: args.id_file = {getattr(args, 'id_file', None)}")
    print(f"ğŸ” DEBUG: args.token = '{args.token}'")
    print(f"ğŸ” DEBUG: len(args.token) = {len(args.token) if args.token else 0}")
    
    return args

def read_ids_from_file(filepath: str) -> list:
    """
    Äá»c IDs tá»« file
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            ids = [line.strip() for line in f if line.strip()]
        return ids
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {filepath}: {e}")
        return []

def main():
    """
    HÃ m main
    """
    print("ğŸš€ DEBUG: Starting main function...")
    
    try:
        args = parse_arguments()
    except SystemExit as e:
        print(f"âŒ DEBUG: SystemExit caught during argument parsing: {e}")
        print(f"ğŸ” DEBUG: Exit code: {e.code}")
        raise
    except Exception as e:
        print(f"âŒ DEBUG: Exception during argument parsing: {e}")
        print(f"ğŸ” DEBUG: Exception type: {type(e).__name__}")
        raise
    
    print("âœ… DEBUG: Arguments parsed successfully")
    
    # Validate token
    print("ğŸ” DEBUG: Validating token...")
    if not args.token:
        print("âŒ ERROR: Token is None or empty")
        sys.exit(1)
    elif args.token == '{{token}}':
        print("âš ï¸ WARNING: Token appears to be a placeholder '{{token}}'")
        print("   This should be replaced with your actual API token")
        print("   Examples:")
        print("   - python main.py --ids 8532 --token your_actual_token_here")
        print("   - export TOKEN=your_token && python main.py --ids 8532 --token $TOKEN")
        # Continue anyway for testing - you may want to change this behavior
        print("   Continuing with placeholder token for debugging...")
    elif args.token.strip() == '':
        print("âŒ ERROR: Token is empty or contains only whitespace")
        sys.exit(1)
    else:
        print(f"âœ… DEBUG: Token appears valid (length: {len(args.token)})")
    
    # Láº¥y danh sÃ¡ch IDs
    print("ğŸ” DEBUG: Processing conversation IDs...")
    if args.ids:
        conversation_ids = args.ids
        print(f"âœ… DEBUG: Using IDs from command line: {conversation_ids}")
    elif args.id_file:
        conversation_ids = read_ids_from_file(args.id_file)
        if not conversation_ids:
            print("âŒ KhÃ´ng thá»ƒ Ä‘á»c IDs tá»« file")
            sys.exit(1)
        print(f"âœ… DEBUG: Using IDs from file: {conversation_ids}")
    else:
        print("âŒ Cáº§n cung cáº¥p --ids hoáº·c --id_file")
        sys.exit(1)
    
    # Khá»Ÿi táº¡o vÃ  cháº¡y pipeline
    print("ğŸ” DEBUG: Initializing pipeline...")
    try:
        pipeline = FastResponsePipeline(args.token)
        print("âœ… DEBUG: Pipeline initialized successfully")
        
        results, merged_file = pipeline.run_pipeline(conversation_ids)
        
        if merged_file:
            print(f"\nğŸ‰ SUCCESS: Pipeline completed successfully!")
            print(f"ğŸ“ Final merged file: {merged_file}")
        else:
            print(f"\nâš ï¸ WARNING: Pipeline completed but merge failed!")
            
    except Exception as e:
        print(f"âŒ DEBUG: Error in pipeline: {e}")
        print(f"ğŸ” DEBUG: Exception type: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    # Náº¿u cháº¡y trá»±c tiáº¿p mÃ  khÃ´ng cÃ³ args, dÃ¹ng test IDs
    if len(sys.argv) == 1:
        print("ğŸ§ª CHáº Y CHáº¾ Äá»˜ TEST Vá»šI IDs Máº¶C Äá»ŠNH")
        print("=" * 60)
        test_ids = ["358", "359", "362"]
        
        try:
            pipeline = FastResponsePipeline("{{token}}")
            results, merged_file = pipeline.run_pipeline(test_ids)
            
            if merged_file:
                print(f"\nğŸ‰ TEST THÃ€NH CÃ”NG!")
                print(f"ğŸ“ File merged: {merged_file}")
            else:
                print(f"\nâŒ TEST THáº¤T Báº I: KhÃ´ng merge Ä‘Æ°á»£c!")
                
        except Exception as e:
            print(f"\nâŒ Lá»–I TRONG TEST: {e}")
            import traceback
            traceback.print_exc()
    else:
        main()
