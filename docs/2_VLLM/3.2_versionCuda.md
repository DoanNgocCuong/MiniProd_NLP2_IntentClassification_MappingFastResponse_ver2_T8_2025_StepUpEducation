Perfect! Now I can see the problem clearly:

**Your system:** CUDA Version 12.1 (Driver Version: 530.41.03)
**Container needs:** CUDA 12.8

Your driver is too old for CUDA 12.8. You have two options:

## Option 1: Use CUDA 12.1 container (Recommended)
Modify your `vllm.sh` script to use a CUDA 12.1 compatible image instead of 12.8.

Let me check your current script:
Read file: ./vllm.sh

I found the issue! Your script is using `vllm/vllm-openai:latest` which requires CUDA 12.8.

**Solution:** Change to a CUDA 12.1 compatible version:
Ran tool

Perfect! I've updated your `vllm.sh` script to use `v0.4.2` which is compatible with CUDA 12.1.

Now let's also check what's using port 9090 (from your original question):
```bash
sudo lsof -i :9090
```

## Summary:

**Fixed:** Changed your `vllm.sh` script from `latest` to `v0.4.2` to be compatible with your CUDA 12.1 driver.

**Now you can try running:**
```bash
./vllm.sh
```

**For the port 9090 check:** If you want to see which process is using it, run:
```bash
sudo lsof -i :9090
```

The CUDA compatibility issue should now be resolved!


---

Let me check the current NVIDIA GPU memory usage for you:
```bash
nvidia-smi
```

Based on the previous `nvidia-smi` output you showed, here's your current NVIDIA GPU memory usage:

## GPU Memory Status:

**GPU 0 (RTX 3090):**
- Used: 24,238 MiB / 24,576 MiB (**98.6% full - almost maxed out**)
- Processes: tritonserver, python backend, python

**GPU 1 (RTX 3090):**
- Used: 16,190 MiB / 24,576 MiB (**65.9% used**)
- Processes: Multiple python processes, gunicorn worker

**GPU 2 (RTX 3090):**
- Used: 626 MiB / 24,576 MiB (**2.5% used - mostly free**)
- Process: One python process

## Summary:
- **Total GPU memory:** 73.7 GB (3 x 24.6 GB)
- **Currently used:** ~41 GB 
- **Available:** ~32.7 GB

**GPU 0** is almost completely full, **GPU 1** is moderately used, and **GPU 2** has plenty of free memory available.

If you want to run your vLLM container, GPU 2 would be the best choice since it has the most free memory!