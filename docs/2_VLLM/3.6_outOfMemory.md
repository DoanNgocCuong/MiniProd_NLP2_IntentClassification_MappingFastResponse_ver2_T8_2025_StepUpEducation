```
Anh ơi, em chạy con 1.5B - trên server 24GB (còn trống 8GB) mà cứ báo [rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/worker/cache_engine.py", line 49, in __init__
[rank0]:     self.gpu_cache = self._allocate_kv_cache(self.num_gpu_blocks, "cuda")
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/worker/cache_engine.py", line 64, in _allocate_kv_cache
[rank0]:     torch.empty(kv_cache_shape,
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 584.00 MiB. GPU 
```


```bash
Làm sao đẻ giảm GPU cần dùng hiện tại xuống khoảng 4GB VRAM mà chất lượng khong giản  (chứ hiện tại thừa hẳn 8GBVRam mà ko host được model, cứ kì kì) 
```


---

docker run --runtime nvidia --gpus '"device=2"' \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 30005:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.4.2 \
    --model Qwen/Qwen2.5-1.5B-Instruct \
    --api-key hoailb-vllm \
    --gpu-memory-utilization 0.32 \
    --max-model-len 2048 \
    --max-num-seqs 16 \
    --block-size 16 \
    --swap-space 4

vẫn bị anh ạ 
e 64, in _allocate_kv_cache
[rank0]:     torch.empty(kv_cache_shape,
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 156.00 MiB. GPU 

---Em cảm ơn anh, chắc đợi mấy anh kia finetune xong