## B·∫£ng: C√°c th√†nh ph·∫ßn ·∫£nh h∆∞·ªüng ƒë·∫øn VRAM khi ch·∫°y LLM (MECE)

| **Th√†nh ph·∫ßn**                                                                                     | **So v·ªõi model size (B)**              | **Config ·∫£nh h∆∞·ªüng ch√≠nh & m√¥ t·∫£ chi ti·∫øt**                                                                                                                          |
| -------------------------------------------------------------------------------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Model Weights**<br>üìå `precision=fp16/int4`                                                      | `= 1.0√ó` v·ªõi FP16<br>`~0.25√ó` v·ªõi INT4 | **`precision`**: FP16 = 2 bytes/param, INT4 = \~0.5 bytes/param. <br>Gi·∫£m precision gi√∫p gi·∫£m m·∫°nh VRAM v√† tƒÉng throughput.                                          |
| **KV Cache**<br>üìå `--max-model-len`, `batch_size`                                                 | `~0.2 ‚Äì 0.6√ó`                          | **`--max-model-len`**: sequence length t·ªëi ƒëa (4096 token tƒÉng cache √ó4 so v·ªõi 1024).<br>**`batch_size`**: c√†ng l·ªõn ‚Üí KV Cache c√†ng ph√¨nh ra.                        |
| **Activation Memory**<br>üìå `batch_size`, `model depth`                                            | `~0.2 ‚Äì 0.4√ó`                          | **`batch_size`**: ·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn memory intermediate.<br>**`model depth`**: c√†ng nhi·ªÅu layers ‚Üí tƒÉng memory theo s·ªë l·ªõp √ó head size.<br>*Dao ƒë·ªông t√πy ki·∫øn tr√∫c* |
| **CUDA Runtime Context**<br>üìå *(t·ª± ƒë·ªông)*                                                         | `~0.1 ‚Äì 0.5 GB`                        | CUDA c·∫ßn VRAM cho kernel launch, memory pools...<br>*Th∆∞·ªùng nh·ªè h∆°n 1 GB, dao ƒë·ªông t√πy driver v√† runtime.*                                                           |
| **Framework Buffers**<br>üìå `--enable-cuda-graphs`, tokenizer preload                              | `~0.3 ‚Äì 1.0 GB`                        | **`--enable-cuda-graphs`**: t·∫°o graph tƒÉng t·ªëc nh∆∞ng tƒÉng usage.<br>**Tokenizer preload**: t·ªën RAM khi load tokenizer l√™n GPU.                                       |
| **Prefill & Padding Buffers**<br>üìå `--gpu-memory-utilization`, padding strategy, `PagedAttention` | `~0.2 ‚Äì 0.8 GB`                        | **`--gpu-memory-utilization`**: gi·ªõi h·∫°n m·ª©c VRAM ƒë∆∞·ª£c d√πng.<br>**Smart padding + PagedAttention**: gi√∫p t·ªëi ∆∞u prefill & tr√°nh memory waste.                        |
| **Memory Fragmentation**<br>üìå *(runtime allocator)*                                               | `~5‚Äì10% t·ªïng VRAM`                     | G√¢y l√£ng ph√≠ do ph√¢n b·ªï b·ªô nh·ªõ kh√¥ng li√™n t·ª•c.<br>**PagedAttention**, **memory pool reuse** c√≥ th·ªÉ gi·∫£m fragmentation.                                               |
| **Multi-Process Overhead**<br>üìå `nproc`, `multi-GPU`, `vLLM workers`                              | `~0.5 ‚Äì 2.0 GB`                        | Ch·∫°y nhi·ªÅu model/process tr√™n c√πng GPU g√¢y overhead.<br>M·ªói worker/process c√≥ context ri√™ng: model weights, cache, CUDA streams...                                   |

---

### üìå **B·∫£ng quy ƒë·ªïi nhanh: B ‚Üí FP16 Model Size**

| Params | FP16 size | INT4 size (∆∞·ªõc t√≠nh) |
| ------ | --------- | -------------------- |
| 1B     | \~2 GB    | \~0.5 GB             |
| 3B     | \~6 GB    | \~1.5 GB             |
| 7B     | \~13 GB   | \~3.3 GB             |
| 13B    | \~24 GB   | \~6.0 GB             |

---

### üõ†Ô∏è **T·ªëi ∆∞u h√≥a c√≥ th·ªÉ b·ªï sung th√™m n·∫øu c·∫ßn**:

| T·ªëi ∆∞u h√≥a              | L·ª£i √≠ch ch√≠nh                            |
| ----------------------- | ---------------------------------------- |
| **PagedAttention**      | Gi·∫£m KV cache fragmentation              |
| **Dynamic Memory Pool** | T√°i s·ª≠ d·ª•ng memory, gi·∫£m waste           |
| **Smart Batching**      | T·ªëi ∆∞u prefill & padding                 |
| **CUDA Graphs**         | TƒÉng t·ªëc inference, gi·∫£m launch overhead |

---

