```
Anh ơi, em chạy con 1.5B - trên server 24GB (còn trống 8GB) mà cứ báo [rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/worker/cache_engine.py", line 49, in __init__
[rank0]:     self.gpu_cache = self._allocate_kv_cache(self.num_gpu_blocks, "cuda")
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/worker/cache_engine.py", line 64, in _allocate_kv_cache
[rank0]:     torch.empty(kv_cache_shape,
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 584.00 MiB. GPU 
```


```bash
Làm sao đẻ giảm GPU cần dùng hiện tại xuống khoảng 4GB VRAM mà chất lượng khong giản  (chứ hiện tại thừa hẳn 8GBVRam mà ko host được model, cứ kì kì) 
```