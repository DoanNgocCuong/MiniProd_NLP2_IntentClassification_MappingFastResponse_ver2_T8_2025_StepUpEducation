
## üß† **3 kh√°i ni·ªám c·∫ßn n·∫Øm r√µ**

| Y·∫øu t·ªë       | Ch·ª©c nƒÉng ch√≠nh                | L∆∞u ·ªü ƒë√¢u?       | GPU c√≥ d√πng kh√¥ng? |
| ------------ | ------------------------------ | ---------------- | ------------------ |
| **Storage**  | L∆∞u file model `.bin`, `.pt`   | SSD / HDD        | ‚ùå Kh√¥ng            |
| **CPU RAM**  | N∆°i ƒë·ªçc file v√† x·ª≠ l√Ω t·∫°m th·ªùi | RAM m√°y (DDR4)   | ‚úÖ C√≥ (giao ti·∫øp)   |
| **GPU VRAM** | N∆°i model ‚Äúth·ª±c s·ª± ch·∫°y‚Äù       | VRAM card ƒë·ªì h·ªça | ‚úÖ Ch√≠nh            |

> ‚úÖ **GPU ch·ªâ quan t√¢m ƒë·∫øn VRAM**, kh√¥ng d√πng Storage ƒë·ªÉ ch·∫°y model.

---

## ‚ö° C√¥ng th·ª©c c∆° b·∫£n d·ªÖ nh·ªõ (d·∫°ng thumb-rule)

### üì¶ Storage

```math
Storage = Model size √ó S·ªë version √ó 1.3 
Model size = S·ªë Parameters * S·ªë bytes/parameter, v·ªõi FP16: 2 bytes/parameter
```

> 1.3 l√† h·ªá s·ªë d·ª± ph√≤ng cho logs, checkpoints, tokenizer,...

V√≠ d·ª•: model 3B d√πng FP16
‚Üí Model size ‚âà 6 GB
‚Üí L∆∞u 3 phi√™n b·∫£n (base + 2 fine-tuned):
‚Üí Storage ‚âà `6 √ó 3 √ó 1.3 = ~21.8 GB`

---

### üß† CPU RAM

```math
CPU RAM ‚âà Model size √ó 0.7 + 8 GB
```

> 8 GB l√† h·ªá ƒëi·ªÅu h√†nh + PyTorch + ti·ªÅn x·ª≠ l√Ω

V√≠ d·ª•: Model 3B
‚Üí `CPU RAM = 6 √ó 0.7 + 8 ‚âà ~12 GB`

---

### üñ•Ô∏è GPU VRAM

```math
GPU VRAM ‚âà Model size √ó 1.6
```

Gi·∫£i th√≠ch:

* 1.0: weights
* 0.2‚Äì0.4: activation, kv-cache
* 0.1‚Äì0.2: CUDA + buffer

‚Üí Model 3B: `6 √ó 1.6 ‚âà ~9 GB`

---

## üìä **B·∫£ng tham chi·∫øu ƒë∆°n gi·∫£n**

| Model Name      | Params | Storage | CPU RAM | GPU VRAM | G·ª£i √Ω GPU      |
| --------------- | ------ | ------- | ------- | -------- | -------------- |
| LLaMA 3B (FP16) | 3B     | 5.6 GB  | 12 GB   | 9 GB     | RTX 3060 12GB  |
| LLaMA 7B        | 7B     | 13 GB   | 20 GB   | 18 GB    | RTX 4070 Ti    |
| LLaMA 13B       | 13B    | 24 GB   | 32 GB   | 32 GB    | RTX 4090       |
| LLaMA 70B       | 70B    | 130 GB  | 120 GB+ | 160‚Äì180G | 4√ó RTX 4090    |
| SDXL            | 3.5B   | 14 GB   | 20 GB   | 18 GB    | RTX 4070 Ti    |
| 3B (INT4)       | 3B     | 1.5 GB  | 6 GB    | 3‚Äì4 GB   | RTX 2060, 2070 |

---

## üîé H·ªá th·ªëng ho·∫°t ƒë·ªông th·∫ø n√†o?

```
Storage (SSD)
   ‚Üì
CPU RAM (ƒë·ªçc, parse, chuy·ªÉn ƒë·ªïi format)
   ‚Üì
GPU VRAM (ch·∫°y inference)
```

---

## üîß Tips t·ªëi ∆∞u:

| K·ªπ thu·∫≠t            | Storage | CPU RAM | GPU VRAM  | M√¥ t·∫£ ng·∫Øn g·ªçn           |
| ------------------- | ------- | ------- | --------- | ------------------------ |
| Quantization (INT4) | ‚Üì 70%   | ‚Üì 50%   | ‚Üì 70%     | Nh·∫π h∆°n, ch·∫°y nhanh h∆°n  |
| Model sharding      | ‚Äî       | ‚Üë 10%   | ‚Üì m·ªói GPU | Chia model l√™n nhi·ªÅu GPU |
| Streaming loading   | ‚Üì 10%   | ‚Üì 30%   | ‚Äî         | Load d·∫ßn khi c·∫ßn         |

---

## ‚úÖ L·ªùi khuy√™n th·ª±c t·∫ø cho Model 3B

### üíª Development

* Storage: **20 GB**
* CPU RAM: **16 GB**
* GPU: **RTX 3060 12GB**

### üöÄ Production

* Storage: **50+ GB SSD**
* CPU RAM: **32 GB**
* GPU: **RTX 4070 Ti / 4080**

---

https://github.com/DoanNgocCuong/MiniProj_RAG3.2_RAG6_LegalChatbot_16032025

- Tr∆∞·ªõc l√†m: Llama-3.2-3B-Instruct-Frog
v·ªõi FP16: ~6.5 GB VRAM (Llama Ai Model)

