@src/ 

‚îú‚îÄ‚îÄ üìÇ src/                          # Core application code
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ qwen_finetune/            # Main package
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ serving/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ setup.py                  # Package definition
‚îÇ
‚îú‚îÄ‚îÄ üìÇ configs/                      # External configs (DevOps managed)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ training_config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ serving_config.yaml
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/                         # External data (Data team managed)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pika_data.json
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ chat_template.txt
‚îÇ
‚îú‚îÄ‚îÄ üìÇ models/                       # Model artifacts (ML team managed)
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ pretrained/
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ merged/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ deployment/                   # Infrastructure (DevOps managed)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ üìÇ scripts/                      # Automation scripts
‚îî‚îÄ‚îÄ üìÑ Makefile                      # Project automation


===

Cho code finetune: 
vllm Qwen2.5, Qwen3, lora nh√© , finetune_unsloth_chatml.py


T√¥i s·∫Ω t·∫°o code Python cho vi·ªác finetune Qwen2.5/Qwen3 v·ªõi vLLM, LoRA v√† unsloth theo c·∫•u tr√∫c th∆∞ m·ª•c b·∫°n ƒë√£ cung c·∫•p.

## 1. Main Training Script - `src/qwen_finetune/training/finetune_unsloth_chatml.py`

```python
#!/usr/bin/env python3
"""
Fine-tune Qwen2.5/Qwen3 using Unsloth and LoRA
Support for ChatML format and vLLM serving
"""

import os
import json
import yaml
import torch
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from transformers import TrainingArguments
from datasets import Dataset, load_dataset
from unsloth import FastLanguageModel, is_bfloat16_supported
from unsloth.chat_templates import get_chat_template
from trl import SFTTrainer
import wandb

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class FineTuneConfig:
    """Configuration for fine-tuning"""
    model_name: str = "unsloth/Qwen2.5-7B-Instruct-bnb-4bit"
    max_seq_length: int = 2048
    dtype: Optional[torch.dtype] = None
    load_in_4bit: bool = True
    
    # LoRA parameters
    r: int = 16
    target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ])
    lora_alpha: int = 16
    lora_dropout: float = 0.0
    bias: str = "none"
    use_gradient_checkpointing: str = "unsloth"
    random_state: int = 3407
    use_rslora: bool = False
    loftq_config: Optional[Dict] = None
    
    # Training parameters
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 4
    warmup_steps: int = 5
    max_steps: int = 60
    learning_rate: float = 2e-4
    fp16: bool = not is_bfloat16_supported()
    bf16: bool = is_bfloat16_supported()
    logging_steps: int = 1
    optim: str = "adamw_8bit"
    weight_decay: float = 0.01
    lr_scheduler_type: str = "linear"
    seed: int = 3407
    output_dir: str = "outputs"
    
    # Data parameters
    dataset_text_field: str = "text"
    packing: bool = False
    
    # Model saving
    save_model: bool = True
    save_method: str = "merged_16bit"  # or "lora"
    push_to_hub: bool = False
    
    # Wandb
    use_wandb: bool = False
    wandb_project: str = "qwen-finetune"

class QwenFineTuner:
    """Fine-tuner for Qwen models using Unsloth"""
    
    def __init__(self, config: FineTuneConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
    def load_model_and_tokenizer(self):
        """Load model and tokenizer with LoRA configuration"""
        logger.info(f"Loading model: {self.config.model_name}")
        
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.model_name,
            max_seq_length=self.config.max_seq_length,
            dtype=self.config.dtype,
            load_in_4bit=self.config.load_in_4bit,
        )
        
        # Add LoRA adapters
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.config.r,
            target_modules=self.config.target_modules,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias=self.config.bias,
            use_gradient_checkpointing=self.config.use_gradient_checkpointing,
            random_state=self.config.random_state,
            use_rslora=self.config.use_rslora,
            loftq_config=self.config.loftq_config,
        )
        
        logger.info("Model and tokenizer loaded successfully")
        
    def prepare_dataset(self, data_path: str, template_path: str) -> Dataset:
        """Prepare dataset with ChatML format"""
        logger.info(f"Loading data from: {data_path}")
        
        # Load data
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # Load chat template
        with open(template_path, 'r', encoding='utf-8') as f:
            chat_template = f.read().strip()
            
        # Set chat template
        self.tokenizer.chat_template = chat_template
        
        # Format conversations
        def formatting_prompts_func(examples):
            convos = examples["conversations"]
            texts = []
            for convo in convos:
                # Convert to ChatML format
                text = self.tokenizer.apply_chat_template(
                    convo, 
                    tokenize=False, 
                    add_generation_prompt=False
                )
                texts.append(text)
            return {"text": texts}
        
        # Create dataset
        dataset = Dataset.from_list(data)
        dataset = dataset.map(
            formatting_prompts_func, 
            batched=True,
            remove_columns=dataset.column_names
        )
        
        logger.info(f"Dataset prepared with {len(dataset)} samples")
        return dataset
        
    def create_trainer(self, dataset: Dataset) -> SFTTrainer:
        """Create SFT trainer"""
        training_args = TrainingArguments(
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            warmup_steps=self.config.warmup_steps,
            max_steps=self.config.max_steps,
            learning_rate=self.config.learning_rate,
            fp16=self.config.fp16,
            bf16=self.config.bf16,
            logging_steps=self.config.logging_steps,
            optim=self.config.optim,
            weight_decay=self.config.weight_decay,
            lr_scheduler_type=self.config.lr_scheduler_type,
            seed=self.config.seed,
            output_dir=self.config.output_dir,
            report_to="wandb" if self.config.use_wandb else None,
        )
        
        self.trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            train_dataset=dataset,
            dataset_text_field=self.config.dataset_text_field,
            max_seq_length=self.config.max_seq_length,
            dataset_num_proc=2,
            packing=self.config.packing,
            args=training_args,
        )
        
        return self.trainer
        
    def train(self):
        """Start training"""
        if self.config.use_wandb:
            wandb.init(project=self.config.wandb_project)
            
        logger.info("Starting training...")
        trainer_stats = self.trainer.train()
        
        logger.info("Training completed!")
        logger.info(f"Training stats: {trainer_stats}")
        
        return trainer_stats
        
    def save_model(self):
        """Save the fine-tuned model"""
        if not self.config.save_model:
            return
            
        logger.info(f"Saving model using method: {self.config.save_method}")
        
        if self.config.save_method == "lora":
            self.model.save_pretrained("lora_model")
            self.tokenizer.save_pretrained("lora_model")
        elif self.config.save_method == "merged_16bit":
            self.model.save_pretrained_merged(
                "models/merged", 
                self.tokenizer, 
                save_method="merged_16bit"
            )
        elif self.config.save_method == "merged_4bit":
            self.model.save_pretrained_merged(
                "models/merged", 
                self.tokenizer, 
                save_method="merged_4bit"
            )
            
        if self.config.push_to_hub:
            self.model.push_to_hub_merged(
                "your-username/your-model-name",
                self.tokenizer,
                save_method=self.config.save_method,
                token="your-huggingface-token"
            )
            
        logger.info("Model saved successfully!")

def load_config(config_path: str) -> FineTuneConfig:
    """Load configuration from YAML file"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    return FineTuneConfig(**config_dict)

def main():
    """Main training function"""
    # Load configuration
    config = load_config("configs/training_config.yaml")
    
    # Initialize fine-tuner
    fine_tuner = QwenFineTuner(config)
    
    # Load model and tokenizer
    fine_tuner.load_model_and_tokenizer()
    
    # Prepare dataset
    dataset = fine_tuner.prepare_dataset(
        "data/pika_data.json",
        "data/chat_template.txt"
    )
    
    # Create trainer
    fine_tuner.create_trainer(dataset)
    
    # Train
    fine_tuner.train()
    
    # Save model
    fine_tuner.save_model()
    
    logger.info("Fine-tuning process completed!")

if __name__ == "__main__":
    main()
```

## 2. Serving Module - `src/qwen_finetune/serving/vllm_server.py`

```python
#!/usr/bin/env python3
"""
vLLM serving server for fine-tuned Qwen models
"""

import asyncio
import json
import yaml
from typing import List, Dict, Optional
from dataclasses import dataclass
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams
from vllm.utils import random_uuid
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ServingConfig:
    """Configuration for vLLM serving"""
    model_path: str = "models/merged"
    host: str = "0.0.0.0"
    port: int = 8000
    
    # vLLM parameters
    tensor_parallel_size: int = 1
    gpu_memory_utilization: float = 0.9
    max_model_len: int = 2048
    dtype: str = "auto"
    quantization: Optional[str] = None
    max_num_seqs: int = 256
    max_num_batched_tokens: int = 2048
    
    # Sampling defaults
    temperature: float = 0.7
    top_p: float = 0.8
    top_k: int = 20
    max_tokens: int = 512

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    max_tokens: Optional[int] = None
    stream: bool = False

class ChatResponse(BaseModel):
    id: str
    choices: List[Dict]
    usage: Dict

class QwenVLLMServer:
    """vLLM server for Qwen models"""
    
    def __init__(self, config: ServingConfig):
        self.config = config
        self.engine = None
        self.app = FastAPI(title="Qwen vLLM Server")
        
        # Add CORS middleware
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        self.setup_routes()
        
    async def initialize_engine(self):
        """Initialize vLLM engine"""
        logger.info("Initializing vLLM engine...")
        
        engine_args = AsyncEngineArgs(
            model=self.config.model_path,
            tensor_parallel_size=self.config.tensor_parallel_size,
            gpu_memory_utilization=self.config.gpu_memory_utilization,
            max_model_len=self.config.max_model_len,
            dtype=self.config.dtype,
            quantization=self.config.quantization,
            max_num_seqs=self.config.max_num_seqs,
            max_num_batched_tokens=self.config.max_num_batched_tokens,
        )
        
        self.engine = AsyncLLMEngine.from_engine_args(engine_args)
        logger.info("vLLM engine initialized successfully!")
        
    def setup_routes(self):
        """Setup API routes"""
        
        @self.app.on_event("startup")
        async def startup_event():
            await self.initialize_engine()
            
        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy"}
            
        @self.app.post("/v1/chat/completions", response_model=ChatResponse)
        async def chat_completions(request: ChatRequest):
            return await self.handle_chat_request(request)
            
    async def handle_chat_request(self, request: ChatRequest) -> ChatResponse:
        """Handle chat completion request"""
        try:
            # Format messages to prompt
            prompt = self.format_messages(request.messages)
            
            # Create sampling parameters
            sampling_params = SamplingParams(
                temperature=request.temperature or self.config.temperature,
                top_p=request.top_p or self.config.top_p,
                top_k=request.top_k or self.config.top_k,
                max_tokens=request.max_tokens or self.config.max_tokens,
            )
            
            # Generate response
            request_id = random_uuid()
            results = self.engine.generate(prompt, sampling_params, request_id)
            
            # Process results
            final_output = None
            async for request_output in results:
                final_output = request_output
                
            if final_output is None:
                raise HTTPException(status_code=500, detail="Generation failed")
                
            # Format response
            response = ChatResponse(
                id=request_id,
                choices=[{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": final_output.outputs[0].text
                    },
                    "finish_reason": final_output.outputs[0].finish_reason
                }],
                usage={
                    "prompt_tokens": len(final_output.prompt_token_ids),
                    "completion_tokens": len(final_output.outputs[0].token_ids),
                    "total_tokens": len(final_output.prompt_token_ids) + len(final_output.outputs[0].token_ids)
                }
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error handling chat request: {e}")
            raise HTTPException(status_code=500, detail=str(e))
            
    def format_messages(self, messages: List[ChatMessage]) -> str:
        """Format messages to ChatML prompt"""
        prompt = ""
        for message in messages:
            prompt += f"<|im_start|>{message.role}\n{message.content}<|im_end|>\n"
        prompt += "<|im_start|>assistant\n"
        return prompt
        
    def run(self):
        """Run the server"""
        logger.info(f"Starting server on {self.config.host}:{self.config.port}")
        uvicorn.run(
            self.app,
            host=self.config.host,
            port=self.config.port,
            log_level="info"
        )

def load_serving_config(config_path: str) -> ServingConfig:
    """Load serving configuration"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    return ServingConfig(**config_dict)

def main():
    """Main serving function"""
    config = load_serving_config("configs/serving_config.yaml")
    server = QwenVLLMServer(config)
    server.run()

if __name__ == "__main__":
    main()
```

## 3. Utils Module - `src/qwen_finetune/utils/data_processor.py`

```python
#!/usr/bin/env python3
"""
Data processing utilities for Qwen fine-tuning
"""

import json
import re
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class DataProcessor:
    """Process data for Qwen fine-tuning"""
    
    @staticmethod
    def convert_to_chatml_format(data: List[Dict], output_path: str):
        """Convert data to ChatML format"""
        processed_data = []
        
        for item in data:
            if "conversations" in item:
                conversations = []
                for turn in item["conversations"]:
                    conversations.append({
                        "role": turn.get("from", "user").replace("human", "user").replace("gpt", "assistant"),
                        "content": turn.get("value", "")
                    })
                processed_data.append({"conversations": conversations})
            elif "instruction" in item:
                # Convert instruction format
                conversations = [
                    {"role": "user", "content": item["instruction"]},
                    {"role": "assistant", "content": item.get("output", "")}
                ]
                if "input" in item and item["input"]:
                    conversations[0]["content"] += f"\n\n{item['input']}"
                processed_data.append({"conversations": conversations})
                
        # Save processed data
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"Processed {len(processed_data)} samples to {output_path}")
        
    @staticmethod
    def validate_data(data_path: str) -> bool:
        """Validate data format"""
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if not isinstance(data, list):
                logger.error("Data should be a list")
                return False
                
            for i, item in enumerate(data):
                if "conversations" not in item:
                    logger.error(f"Item {i} missing 'conversations' field")
                    return False
                    
                conversations = item["conversations"]
                if not isinstance(conversations, list):
                    logger.error(f"Item {i} 'conversations' should be a list")
                    return False
                    
                for j, turn in enumerate(conversations):
                    if "role" not in turn or "content" not in turn:
                        logger.error(f"Item {i}, turn {j} missing 'role' or 'content'")
                        return False
                        
            logger.info(f"Data validation passed for {len(data)} samples")
            return True
            
        except Exception as e:
            logger.error(f"Data validation failed: {e}")
            return False
            
    @staticmethod
    def create_sample_data(output_path: str, num_samples: int = 10):
        """Create sample training data"""
        sample_data = []
        
        for i in range(num_samples):
            conversations = [
                {
                    "role": "user",
                    "content": f"ƒê√¢y l√† c√¢u h·ªèi m·∫´u s·ªë {i+1}. B·∫°n c√≥ th·ªÉ gi√∫p t√¥i kh√¥ng?"
                },
                {
                    "role": "assistant", 
                    "content": f"Ch√†o b·∫°n! T√¥i r·∫•t s·∫µn l√≤ng gi√∫p b·∫°n v·ªõi c√¢u h·ªèi s·ªë {i+1}. ƒê√¢y l√† c√¢u tr·∫£ l·ªùi m·∫´u."
                }
            ]
            sample_data.append({"conversations": conversations})
            
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"Created {num_samples} sample conversations in {output_path}")

def main():
    """Test data processor"""
    processor = DataProcessor()
    
    # Create sample data
    processor.create_sample_data("data/pika_data.json", 20)
    
    # Validate data
    processor.validate_data("data/pika_data.json")

if __name__ == "__main__":
    main()
```

## 4. Configuration Files

### `configs/training_config.yaml`

```yaml
# Model configuration
model_name: "unsloth/Qwen2.5-7B-Instruct-bnb-4bit"  # or "unsloth/Qwen2.5-14B-Instruct-bnb-4bit"
max_seq_length: 2048
load_in_4bit: true

# LoRA configuration
r: 16
target_modules:
  - "q_proj"
  - "k_proj" 
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
lora_alpha: 16
lora_dropout: 0.0
bias: "none"
use_gradient_checkpointing: "unsloth"
use_rslora: false

# Training configuration
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
warmup_steps: 5
max_steps: 60
learning_rate: 2.0e-4
weight_decay: 0.01
lr_scheduler_type: "linear"
optim: "adamw_8bit"
seed: 3407

# Data configuration
dataset_text_field: "text"
packing: false

# Saving configuration
save_model: true
save_method: "merged_16bit"  # "lora", "merged_16bit", "merged_4bit"
push_to_hub: false
output_dir: "outputs"

# Logging
use_wandb: false
wandb_project: "qwen-finetune"
logging_steps: 1
```

### `configs/serving_config.yaml`

```yaml
# Model configuration
model_path: "models/merged"

# Server configuration  
host: "0.0.0.0"
port: 8000

# vLLM configuration
tensor_parallel_size: 1
gpu_memory_utilization: 0.9
max_model_len: 2048
dtype: "auto"
quantization: null
max_num_seqs: 256
max_num_batched_tokens: 2048

# Sampling defaults
temperature: 0.7
top_p: 0.8
top_k: 20
max_tokens: 512
```

### `data/chat_template.txt`

```
{% for message in messages %}
{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}
{% endfor %}
{% if add_generation_prompt %}
{{'<|im_start|>assistant\n'}}
{% endif %}
```

## 5. Package Setup - `src/setup.py`

```python
from setuptools import setup, find_packages

setup(
    name="qwen-finetune",
    version="0.1.0",
    description="Fine-tuning toolkit for Qwen models with vLLM and LoRA",
    packages=find_packages(),
    install_requires=[
        "torch>=2.0.0",
        "transformers>=4.40.0",
        "datasets>=2.18.0",
        "accelerate>=0.28.0",
        "peft>=0.10.0",
        "trl>=0.8.0",
        "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git",
        "vllm>=0.4.0",
        "fastapi>=0.100.0",
        "uvicorn>=0.22.0",
        "pydantic>=2.0.0",
        "pyyaml>=6.0",
        "wandb>=0.15.0",
    ],
    python_requires=">=3.8",
    author="Your Name",
    author_email="your.email@example.com",
    url="https://github.com/yourusername/qwen-finetune",
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
    ],
)
```

## 6. Scripts v√† Automation

### `scripts/train.sh`

```bash
#!/bin/bash
# Training script

set -e

echo "Starting Qwen fine-tuning..."

# Activate environment
source venv/bin/activate

# Set environment variables
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Run training
python src/qwen_finetune/training/finetune_unsloth_chatml.py

echo "Training completed!"
```

### `scripts/serve.sh`

```bash
#!/bin/bash
# Serving script

set -e

echo "Starting vLLM server..."

# Activate environment
source venv/bin/activate

# Set environment variables
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"

# Run server
python src/qwen_finetune/serving/vllm_server.py

echo "Server started!"
```

### `Makefile`

```makefile
.PHONY: install train serve clean test

# Install dependencies
install:
	pip install -r requirements.txt
	pip install -e src/

# Create sample data
data:
	python -c "from src.qwen_finetune.utils.data_processor import DataProcessor; DataProcessor.create_sample_data('data/pika_data.json', 50)"

# Run training
train:
	bash scripts/train.sh

# Run serving
serve:
	bash scripts/serve.sh

# Test the setup
test:
	python -c "from src.qwen_finetune.utils.data_processor import DataProcessor; DataProcessor.validate_data('data/pika_data.json')"
	echo "Setup test passed!"

# Clean up
clean:
	rm -rf outputs/
	rm -rf models/merged/
	rm -rf __pycache__/
	find . -name "*.pyc" -delete

# Setup environment
setup:
	python -m venv venv
	source venv/bin/activate && pip install --upgrade pip
	source venv/bin/activate && make install
	make data
	echo "Setup completed!"
```

## 7. Docker Configuration

### `deployment/Dockerfile`

```dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu22.04

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    git \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy source code
COPY src/ ./src/
COPY configs/ ./configs/
COPY data/ ./data/
COPY scripts/ ./scripts/

# Install package
RUN pip3 install -e src/

# Expose port
EXPOSE 8000

# Default command
CMD ["python3", "src/qwen_finetune/serving/vllm_server.py"]
```

### `deployment/docker-compose.yml`

```yaml
version: '3.8'

services:
  qwen-finetune:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ../models:/app/models
      - ../data:/app/data
      - ../configs:/app/configs
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app/src
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
```

## C√°ch s·ª≠ d·ª•ng:

1. **Setup m√¥i tr∆∞·ªùng:**
```bash
make setup
```

2. **T·∫°o d·ªØ li·ªáu m·∫´u:**
```bash
make data
```

3. **Fine-tuning:**
```bash
make train
```

4. **Serving:**
```bash
make serve
```

5. **Test API:**
```bash
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Xin ch√†o!"}],
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

C·∫•u tr√∫c n√†y cung c·∫•p m·ªôt framework ho√†n ch·ªânh ƒë·ªÉ fine-tune Qwen2.5/Qwen3 v·ªõi unsloth v√† LoRA, sau ƒë√≥ serve b·∫±ng vLLM v·ªõi hi·ªáu su·∫•t cao!