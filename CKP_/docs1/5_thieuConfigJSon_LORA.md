## ğŸ¤” **Táº¡i sao thiáº¿u config.json?**

Váº¥n Ä‘á» nÃ y xuáº¥t phÃ¡t tá»« **cÃ¡ch Unsloth save checkpoint** vÃ  **cÃ¡ch vLLM load model**:

### **ğŸ” Root cause:**

1. **Unsloth checkpoint** chá»‰ lÆ°u **LoRA adapter weights**, khÃ´ng lÆ°u full model config
2. **vLLM cáº§n full model** vá»›i Ä‘áº§y Ä‘á»§ files: `config.json`, `tokenizer.json`, etc.
3. **Export script báº¡n viáº¿t** cÃ³ váº¥n Ä‘á» á»Ÿ viá»‡c load tá»« checkpoint

---

## ğŸ› ï¸ **Fix export script:**

Váº¥n Ä‘á» á»Ÿ Ä‘Ã¢y:

```python
# âŒ WRONG: Load tá»« checkpoint (chá»‰ cÃ³ LoRA weights)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=CHECKPOINT_PATH,  # checkpoint-62 chá»‰ cÃ³ LoRA
    ...
)
```

**âœ… CORRECT approach:**

```python
#!/usr/bin/env python3
"""
Fixed export script - Load base model + apply LoRA checkpoint
"""

from unsloth import FastLanguageModel
from peft import PeftModel
import torch
import os

# Configuration
MODEL_NAME = "unsloth/Qwen3-1.7B"
CHECKPOINT_PATH = "./pretrained_models/unsloth_chatml_finetuned/checkpoint-62"
OUTPUT_PATH = "./pretrained_models/exported_full_model"
max_seq_length = 2048

def export_model():
    print("ğŸ”„ Loading base model...")
    
    # 1. Load base model (cÃ³ Ä‘áº§y Ä‘á»§ config)
    base_model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,  # âœ… Load base model trÆ°á»›c
        max_seq_length=max_seq_length,
        dtype=None,
        load_in_4bit=False,
    )
    
    # 2. Apply LoRA weights tá»« checkpoint
    print("ğŸ”„ Applying LoRA checkpoint...")
    model = FastLanguageModel.get_peft_model(
        base_model,
        r=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                       "gate_proj", "up_proj", "down_proj"],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
    )
    
    # 3. Load trained weights tá»« checkpoint
    print(f"ğŸ”„ Loading checkpoint: {CHECKPOINT_PATH}")
    # Cáº§n load state dict tá»« checkpoint
    checkpoint_files = os.listdir(CHECKPOINT_PATH)
    print(f"ğŸ“ Files in checkpoint: {checkpoint_files}")
    
    # Load adapter weights (if exists)
    adapter_path = os.path.join(CHECKPOINT_PATH, "adapter_model.safetensors")
    if os.path.exists(adapter_path):
        from safetensors.torch import load_file
        adapter_weights = load_file(adapter_path)
        model.load_state_dict(adapter_weights, strict=False)
    
    # 4. Merge LoRA vÃ  export
    print("ğŸ”„ Merging LoRA weights...")
    model = FastLanguageModel.for_inference(model)
    
    # 5. Save merged model
    print(f"ğŸ’¾ Saving to: {OUTPUT_PATH}")
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    
    model.save_pretrained(OUTPUT_PATH, safe_serialization=True)
    tokenizer.save_pretrained(OUTPUT_PATH)
    
    # 6. Verify files
    required_files = ['config.json', 'tokenizer_config.json', 'model.safetensors']
    for file in required_files:
        file_path = os.path.join(OUTPUT_PATH, file)
        if os.path.exists(file_path):
            print(f"âœ… {file} - OK")
        else:
            print(f"âŒ {file} - MISSING")
    
    print("ğŸ‰ Export completed!")

if __name__ == "__main__":
    export_model()
```

---

## ğŸ”§ **Alternative: Sá»­ dá»¥ng Unsloth built-in export:**

```python
#!/usr/bin/env python3

from unsloth import FastLanguageModel
import os

MODEL_NAME = "unsloth/Qwen3-1.7B"
LORA_OUTPUT = "./pretrained_models/unsloth_chatml_finetuned"
MERGED_OUTPUT = "./pretrained_models/exported_full_model"

def export_with_unsloth():
    # Load model + LoRA tá»« thÆ° má»¥c output (khÃ´ng pháº£i checkpoint)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=LORA_OUTPUT,  # âœ… Load tá»« thÆ° má»¥c chÃ­nh, khÃ´ng pháº£i checkpoint
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=False,
    )
    
    # Merge vÃ  save
    print("ğŸ”„ Merging and saving...")
    model.save_pretrained_merged(
        MERGED_OUTPUT,
        tokenizer,
        save_method="merged_16bit",  # hoáº·c "merged_4bit" náº¿u cáº§n
    )
    
    print("âœ… Done with Unsloth export!")

if __name__ == "__main__":
    export_with_unsloth()
```

---

## ğŸ“Š **Giáº£i thÃ­ch táº¡i sao xáº£y ra:**

| Component | CÃ³ trong LoRA Checkpoint? | CÃ³ trong Base Model? | Cáº§n cho vLLM? |
|-----------|---------------------------|---------------------|---------------|
| `config.json` | âŒ | âœ… | âœ… **Required** |
| `tokenizer.json` | âŒ | âœ… | âœ… **Required** |
| `model weights` | ğŸ”¸ Partial (chá»‰ LoRA) | âœ… Full | âœ… **Full weights** |
| `generation_config.json` | âŒ | âœ… | âœ… **Required** |

**â†’ Checkpoint chá»‰ lÆ°u delta weights, khÃ´ng lÆ°u full model config!**

---

## ğŸ¯ **Quick fix - Thá»­ approach nÃ y:**

```bash
# 1. Kiá»ƒm tra xem cÃ³ file nÃ o trong checkpoint
ls -la ./pretrained_models/unsloth_chatml_finetuned/

# 2. Náº¿u cÃ³ adapter_config.json, sá»­ dá»¥ng PEFT merge:
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("unsloth/Qwen3-1.7B")
tokenizer = AutoTokenizer.from_pretrained("unsloth/Qwen3-1.7B")

# Load LoRA adapter
model = PeftModel.from_pretrained(
    base_model, 
    "./pretrained_models/unsloth_chatml_finetuned"
)

# Merge
merged_model = model.merge_and_unload()

# Save
merged_model.save_pretrained("./pretrained_models/exported_full_model")
tokenizer.save_pretrained("./pretrained_models/exported_full_model")
```

**Báº£n cháº¥t**: LoRA fine-tune chá»‰ train adapter weights, cáº§n merge vá»›i base model Ä‘á»ƒ cÃ³ full config! ğŸ¯