# ğŸš€ vLLM API - Curl Commands Reference

## ğŸ“‹ Server Configuration
- **Port**: 30005
- **API Key**: hoailb-vllm
- **Model**: Qwen/Qwen2.5-1.5B-Instruct
- **Max Tokens**: 1024
- **Max Concurrent Requests**: 16

---

## ğŸ”¥ Test API cÆ¡ báº£n (Health check)
```bash
curl http://localhost:30005/health
```

---

## ğŸ’¬ Chat Completion (Khuyáº¿n nghá»‹ - giá»‘ng ChatGPT)
```bash
curl -X POST "http://localhost:30005/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer hoailb-vllm" \
  -d '{
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "messages": [
      {"role": "user", "content": "Xin chÃ o! Báº¡n cÃ³ thá»ƒ giÃºp tÃ´i khÃ´ng?"}
    ],
    "max_tokens": 512,
    "temperature": 0.7
  }'
```

---

## ğŸ“ Text Completion (ÄÆ¡n giáº£n hÆ¡n)
```bash
curl -X POST "http://localhost:30005/v1/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer hoailb-vllm" \
  -d '{
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "prompt": "Viáº¿t má»™t bÃ i thÆ¡ ngáº¯n vá» mÃ¹a thu:",
    "max_tokens": 256,
    "temperature": 0.8
  }'
```

---

## ğŸ“‹ Liá»‡t kÃª models cÃ³ sáºµn
```bash
curl -H "Authorization: Bearer hoailb-vllm" \
  http://localhost:30005/v1/models
```

---

## ğŸ® Test streaming response (real-time)
```bash
curl -X POST "http://localhost:30005/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer hoailb-vllm" \
  -d '{
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "messages": [
      {"role": "user", "content": "Giáº£i thÃ­ch vá» AI trong 3 cÃ¢u"}
    ],
    "max_tokens": 200,
    "stream": true
  }' --no-buffer
```

---

## ğŸ”§ Parameters quan trá»ng

| Parameter | Range | Description |
|-----------|-------|-------------|
| `temperature` | 0.0-2.0 | CÃ ng cao cÃ ng creative |
| `max_tokens` | 1-1024 | Sá»‘ token tá»‘i Ä‘a (theo config) |
| `top_p` | 0.0-1.0 | Nucleus sampling |
| `frequency_penalty` | -2.0 to 2.0 | TrÃ¡nh láº·p tá»« |
| `presence_penalty` | -2.0 to 2.0 | Khuyáº¿n khÃ­ch chá»§ Ä‘á» má»›i |

---

## ğŸ¯ Quick Test Commands

### Test 1: Basic Health Check
```bash
curl http://localhost:30005/health
```

### Test 2: Simple Question
```bash
curl -X POST "http://localhost:30005/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer hoailb-vllm" \
  -d '{
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "messages": [{"role": "user", "content": "Hello, what is 2+2?"}],
    "max_tokens": 100
  }'
```

### Test 3: Vietnamese Language
```bash
curl -X POST "http://localhost:30005/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer hoailb-vllm" \
  -d '{
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "messages": [{"role": "user", "content": "Viá»‡t Nam cÃ³ bao nhiÃªu tá»‰nh thÃ nh?"}],
    "max_tokens": 200,
    "temperature": 0.3
  }'
```

---

## ğŸ“Š Expected Response Format

### Chat Completion Response:
```json
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "Qwen/Qwen2.5-1.5B-Instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Response text here..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 50,
    "total_tokens": 60
  }
}
```

---

## âš ï¸ Troubleshooting

1. **Connection refused**: Kiá»ƒm tra container cÃ³ Ä‘ang cháº¡y khÃ´ng
2. **401 Unauthorized**: Kiá»ƒm tra API key `hoailb-vllm`
3. **404 Not Found**: Kiá»ƒm tra endpoint URL
4. **500 Internal Error**: Kiá»ƒm tra logs container
5. **Timeout**: CÃ³ thá»ƒ model Ä‘ang load, Ä‘á»£i 1-2 phÃºt

### Check container status:
```bash
docker ps | grep vllm
```

### Check logs:
```bash
docker logs <container_id>
```

---

*Generated on: $(date)*
*Server: vLLM v0.4.2 with Qwen2.5-1.5B-Instruct*