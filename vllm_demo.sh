# =============================================================================
# ğŸš€ VLLM CONFIG CHI TIáº¾T - QWEN 2.5-1.5B OPTIMIZATION
# =============================================================================

# ğŸ¯ GPU Selection: Chá»‰ dÃ¹ng GPU 2 (cÃ³ ~24GB trá»‘ng)
# VÃ­ dá»¥: GPU 0 (98% full), GPU 1 (65% used) â†’ Chá»n GPU 2 (2.5% used)

# ğŸ’¾ Mount HuggingFace cache Ä‘á»ƒ khÃ´ng download láº¡i model
# ğŸŒ Port mapping: Host:30005 â†’ Container:8000  
# ğŸ”— Shared memory cho multi-processing
# ğŸ“¦ Container version: v0.4.2 (compatible vá»›i CUDA 12.1)

# ğŸ¤– Model: 1.5B parameters - Memory footprint: ~3GB (FP16/BF16)
# ğŸ” API authentication key

# ğŸ§  GPU Memory Usage: 32% cá»§a available memory (conservative)
# VÃ­ dá»¥: GPU cÃ³ 24GB â†’ vLLM dÃ¹ng tá»‘i Ä‘a 7.7GB
# 0.3 = 7.2GB, 0.5 = 12GB, 0.8 = 19.2GB, 0.9 = 21.6GB
# Trade-off: Tháº¥p hÆ¡n = an toÃ n hÆ¡n nhÆ°ng underutilize GPU

# ğŸ“ Max Sequence Length: 2048 tokens
# KV Cache memory = seq_len Ã— hidden_size Ã— layers Ã— 2 Ã— batch  
# VÃ­ dá»¥ Qwen-1.5B: 2048 Ã— 1536 Ã— 28 Ã— 2 Ã— 16 = ~2.8GB
# 1024â†’~1.4GB | 2048â†’~2.8GB | 4096â†’~5.6GB | 8192â†’~11.2GB

# ğŸ‘¥ Max Concurrent Requests: 16 requests Ä‘á»“ng thá»i
# Memory per request = KV_cache_size / max_num_seqs
# 8 requestsâ†’350MB/req | 16 requestsâ†’175MB/req | 32 requestsâ†’87MB/req

# ğŸ§± KV Cache Block Size: 16 tokens/block
# Memory allocation unit = block_size Ã— hidden_size Ã— layers Ã— 2
# 8â†’0.7MB/block | 16â†’1.4MB/block | 32â†’2.8MB/block

# ğŸ’¿ CPU RAM Backup: 4GB CPU RAM cho overflow KV cache
# 0GB=no backup (risk OOM) | 4GB=moderate | 8GB=safe nhÆ°ng cháº­m khi swap

docker run --runtime nvidia --gpus '"device=1"' \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 30005:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.4.2 \
    --model Qwen/Qwen2.5-1.5B-Instruct \
    --api-key hoailb-vllm \
    --gpu-memory-utilization 0.7 \
    --max-model-len 1024 \
    --max-num-seqs 16 \
    --block-size 16 \
    --swap-space 4

# =============================================================================
# ğŸ“Š MEMORY BREAKDOWN SUMMARY:
# Model weights:     ~3.0GB  (Qwen-1.5B FP16)
# KV Cache:          ~2.8GB  (2048 tokens Ã— 16 seqs)  
# Activations:       ~0.5GB  (processing overhead)
# Buffer & CUDA:     ~0.7GB  (vLLM + PyTorch overhead)
# Total GPU VRAM:    ~7.0GB  (vá»›i safety margin)
# =============================================================================